### Установите библиотеки из списка в requirements.txt.


## Пример 
1. Данная версия приложения содержит датасет house_prices:
    источник - https://www.kaggle.com/c/house-prices-advanced-regression-techniques
2. Для запуска AutoML на данном датасете достаточно ввести команду "python process.py" в корневой папке.


## Для корректной работы необходимо произвести следующие действия
1. Действия с файлами
    1.1 Переименовать свой датасет (весь или часть выборки для обучения) в train.csv,
    1.2 Если имеется отдельный датасет с валидационными данным нужно переименовать файл в val.csv,
    1.3 Далее в папке datasets создать папку и поместить в неё файлы из шагов 1) и 2).
2. Настройка конфига (config.py)
    2.1 Настройки params
        "dataset_name" присвоить имя папки созданной на шаге 1.3,
         "target_name" присвоить имя колонки с целевой переменной,
         "index_name" присвоить имя колонки, которая является индексом, можно оставить пустым.
    2.2 Остальный параметры можно оставить по умолчанию, либо обратится к описанию config.py
3. Для запуска расчета моделей необходимо исполнить скрипт process.py:
   python process.py


## Описание config.py
1. Общие параметры
    1.1 path_to_datasets - указывает на путь к папке с датасетами. Менять не требуется
    1.2 path_to_preprocessed_datasets - указывает на путь к папке, в которой будем хранить результаты работы приложения. Менять не требуется
    1.3 save_preprocessed_data - флаг, при значении true разрешает сохранение  подготовленных для обучения датасетов. Может быть полезно при длительной обработки данных.
2. params - общие параметры
    2.1 "dataset_name": - имя папки, в которой лежит используемый набор данных,
    2.2 "task" - имя задачи, например "regression". Данная версия поддерживает только "regression",
    2.3 "target_name" - имя колонки, в которой находится целевая переменная,
    2.4 "features_names" - имена колонок, которые необходимо использовать как признаки, можно передавать пустой массив [], чтобы использовать все признаки,
    2.5 "create_validation_dataset" - флаг, значение True говорит о том, что необходимо создать валидационный датасет из переданного train.csv,
    2.6 "index_name" - имя колонки, которая является индексом.
3. validation_params - параметры описывающие создание валидационного датасета
    3.1 "validation_split_type" - как разделять на тренировочную и валидационную выборки. Варианты значений: ["random", "order"].
        order - разделение по порядку строк.
        random - рандомно разделить данные.
    3.2 "percent_on_train" - какую долю выборки использовать для обучение (например 0.8)


## Необходимые доработки 
1) Добавить обработку текстовых признаков (реальных текстов, например твитов),
2) Добавить решение задач классификации,
3) Добавить учет дисбаланса класса при разбиение на train, val для задач классификации,
4) Обработка признаков дата-время,
5) Организовать процедуру заполнения NaN, сейчас строки с NaN удаляются, при этом производится предварительная очистка колонок с большим количеством NaN,
6) Mean Encoding применяется на все категориальные признаки, хотелось бы обеспечить возможность выбора способ кодирования признаков,
7) Поддержка K-fold
8) Функционал проверки результатов на отложенной тестовой выборке
etc...
